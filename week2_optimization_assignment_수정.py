# -*- coding: utf-8 -*-
"""week2_optimization_assignment.ipynb의 사본

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12oy8LXuiA5BdEVYvvz6gGMKKmOHgA1Vi

# Tobig's 14기 2주차 Optimization 과제
### Made by 이지용

# Gradient Descent 구현하기

### 1) "..." 표시되어 있는 빈 칸을 채워주세요  
### 2) 강의내용과 코드에 대해 공부한 내용을 적어서 과제를 채워주세요
"""

pip install -U scikit-learn

import pandas as pd
import numpy as np
import random
import tensorflow as tf
import math

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/My Drive/투빅스/2주차/assignment_2.csv')
data.head()

data.iloc[:, 1:]

data.iloc[:, 0]

"""## Train Test 데이터 나누기
### 데이터셋을 train/test로 나눠주는 메소드  
https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size=0.25, random_state = 0)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""## Scaling  

experience와 salary의 단위, 평균, 분산이 크게 차이나므로 scaler를 사용해 단위를 맞춰줍니다.
"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
bias_train = X_train["bias"]
bias_train = bias_train.reset_index()["bias"]
X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)
X_train["bias"] = bias_train
X_train.head()

"""이때 scaler는 X_train에 fit 해주시고, fit한 scaler를 X_test에 적용시켜줍니다.  
똑같이 X_test에다 fit하면 안돼요!
"""

bias_test = X_test["bias"]
bias_test = bias_test.reset_index()["bias"]
X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)
X_test["bias"] = bias_test
X_test.head()

# parameter 개수
N = len(X_train.loc[0])

# 초기 parameter들을 임의로 설정해줍니다.
parameters = np.array([random.random() for i in range(N)])
parameters

"""### * LaTeX   

Jupyter Notebook은 LaTeX 문법으로 수식 입력을 지원하고 있습니다.  
http://triki.net/apps/3466  
https://jjycjnmath.tistory.com/117

## Logistic Function

## $p = "\frac{1}{1 + e^{-(\hat{\beta_0}+\hat{\beta_1}x_1+...+\hat{\beta_k}x_k)}}"$
"""

def logistic(X, parameters):
    z = 0
    for i in range(len(parameters)):
      z += parameters[i]*X[i]
    p = 1/(1+np.exp(-z))
    
    return p

logistic(X_train.iloc[1], parameters)

"""## Object Function

Object Function : 목적함수는 Gradient Descent를 통해 최적화 하고자 하는 함수입니다.  
로지스틱 회귀의 목적함수를 작성해주세요
## $l(p) ="-\sum[{(y_i)logp+(1-y_i)log(1-p)}]"$
"""

def minus_log_cross_entropy_i(X, y, parameters) :
  p = logistic(X, parameters)   
  loss = -np.sum(y*np.log(p)+(1-y)*np.log(1-p))
  return loss

def minus_log_cross_entropy(X_set, y_set, parameters) :
    loss = 0
    for i in range(X_set.shape[0]):
        X = X_set.iloc[i, :]
        y = y_set.iloc[i]
        loss += minus_log_cross_entropy_i(X,y,parameters)
    return loss

minus_log_cross_entropy(X_test, y_test, parameters)

"""## Gradient of Minus Log Cross Entropy

## ${\partial\over{\partial \theta_j}}l(p)= "-\sum[(y_i-p_i)x_{ij}]"$
"""

# cross_entropy를 theta_j에 대해 미분한 값을 구하는 함수
def get_gradient_ij_minus_log_cross_entropy(X, y, parameters, j):
    p = logistic(X, parameters)
    gradient = -((y-p)*X[j])
    return gradient

get_gradient_ij_minus_log_cross_entropy(X_train.iloc[0, :], y_train.iloc[0], parameters, 1)

"""## Batch Gradient Descent  

Batch Gradient Descent : 
$w=w-\eta \nabla Q(w)=w-\eta \sum_{i=1}^{n} \nabla Q_{i}(w)$
"""

def get_gradients_bgd(X_train, y_train, parameters) :
    gradients = [0 for i in range(len(parameters))]
    
    for i in range(X_train.shape[0]):
        X = X_train.iloc[i, :]
        y = y_train.iloc[i]
        for j in range(len(parameters)):
            gradients[j] += get_gradient_ij_minus_log_cross_entropy(X, y, parameters, j)
            
    return gradients

gradients_bgd = get_gradients_bgd(X_train, y_train, parameters)
gradients_bgd

"""## Stochastic Gradient Descent  

Stochastic Gradient Descent : $Q(w)=\sum_{i=1}^nQi(w)$
"""

def get_gradients_sgd(X_train, y, parameters) :
    gradients = [0 for i in range(len(parameters))]
    r = int(random.random()*X_train.shape[0])
    X = X_train.iloc[r, :]
    y = y_train.iloc[r]
        
    for j in range(len(parameters)):
        gradients[j] = get_gradient_ij_minus_log_cross_entropy(X, y, parameters, j)
        
    return gradients

gradients_sgd = get_gradients_sgd(X_train, y_train, parameters)
gradients_sgd

"""## Update Parameters"""

def update_parameters(parameters, gradients, learning_rate) :
    for i in range(len(parameters)) :
        gradients[i] *= learning_rate
    parameters -= gradients
    return parameters

update_parameters(parameters, gradients_bgd, 0.01)

"""## Gradient Descent  

위에서 작성한 함수들을 조합해서 Gradient Descent를 진행하는 함수를 완성해주세요

learning_rate = 0.01  
max_iter = 100000  
tolerance = 0.0001
"""

def gradient_descent(X_train, y_train, learning_rate=0.01, max_iter=100000, tolerance=0.0001, optimizer="bgd") :
    count = 1
    point = 100 if optimizer == "bgd" else 10000
    N = len(X_train.iloc[0])
    parameters = np.array([random.random() for i in range(N)])
    gradients = [0 for i in range(N)]
    loss = 0
    
    while count < max_iter :
        
        if optimizer == "bgd" :
            gradients =  get_gradients_bgd(X_train, y_train, parameters)
        elif optimizer == "sgd" :
            gradients = get_gradients_sgd(X_train, y_train, parameters)
            # loss, 중단 확인
        if count%point == 0 :
            new_loss = minus_log_cross_entropy(X_train, y_train, parameters)
            print(count, "loss: ",new_loss, "params: ", parameters, "gradients: ", gradients)
            
            #중단 조건
            if abs(new_loss-loss) < tolerance*len(y_train) :
                break
            loss = new_loss
                
            
                
        parameters = update_parameters(parameters, gradients, learning_rate)
        count += 1
    return parameters

new_param_bgd = gradient_descent(X_train, y_train)
new_param_bgd

"""## Hyper Parameter Tuning

Hyper Parameter들을 매번 다르게 해서 학습을 진행해 보세요. 다른 점들을 발견할 수 있습니다.
"""

new_param_sgd = gradient_descent(X_train, y_train, learning_rate=0.01, max_iter=100000, tolerance=0.0001, optimizer="sgd")
new_param_sgd

"""## Predict Label"""

y_predict = []
for i in range(len(y_test)):
    p = logistic(X_test.iloc[i,:], new_param_bgd)
    if p> 0.5 :
        y_predict.append(1)
    else :
        y_predict.append(0)

"""## Confusion Matrix"""

from sklearn.metrics import *
tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()
confusion_matrix(y_test, y_predict)